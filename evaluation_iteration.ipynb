{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNiW2qjecFfXsf+gkGxN/a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nV9cSOFeN8w","executionInfo":{"status":"ok","timestamp":1709270700370,"user_tz":-330,"elapsed":10,"user":{"displayName":"MUNTAZIR JAHANGIR (RA2111047010138)","userId":"06765373775699398059"}},"outputId":"0b5f4d4c-1ca9-4f92-a1b3-ff653b3624ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy: [0 0 1]\n","Optimal Value Function: [7.67178149 7.7399765  7.60667721]\n"]}],"source":["import numpy as np\n","\n","def policy_iteration(states, actions, transition_prob, rewards, gamma=0.9, epsilon=1e-6):\n","    policy = np.ones(len(states), dtype=int)  # Initialize policy\n","    value_function = np.zeros(len(states))  # Initialize value function\n","\n","    while True:\n","        # Policy Evaluation\n","        while True:\n","            delta = 0\n","            for s in states:\n","                v = value_function[s]\n","                value_function[s] = sum(transition_prob[s, policy[s], s_next] *\n","                                        (rewards[s, policy[s], s_next] + gamma * value_function[s_next])\n","                                        for s_next in states)\n","                delta = max(delta, abs(v - value_function[s]))\n","\n","            if delta < epsilon:\n","                break\n","\n","        # Policy Improvement\n","        policy_stable = True\n","        for s in states:\n","            old_action = policy[s]\n","\n","            # Greedily choose the action that maximizes expected value\n","            policy[s] = np.argmax([sum(transition_prob[s, a, s_next] *\n","                                       (rewards[s, a, s_next] + gamma * value_function[s_next])\n","                                       for s_next in states) for a in actions])\n","\n","            if old_action != policy[s]:\n","                policy_stable = False\n","\n","        if policy_stable:\n","            break\n","\n","    return policy, value_function\n","\n","# Example usage:\n","states = [0, 1, 2]\n","actions = [0, 1]  # Assuming two possible actions\n","transition_prob = np.random.rand(len(states), len(actions), len(states))\n","transition_prob /= transition_prob.sum(axis=-1, keepdims=True)\n","rewards = np.random.rand(len(states), len(actions), len(states))\n","gamma = 0.9\n","\n","optimal_policy, optimal_value_function = policy_iteration(states, actions, transition_prob, rewards, gamma)\n","print(\"Optimal Policy:\", optimal_policy)\n","print(\"Optimal Value Function:\", optimal_value_function)\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","def value_iteration(states, actions, transition_prob, rewards, gamma=0.9, epsilon=1e-6):\n","    value_function = np.zeros(len(states))\n","\n","    while True:\n","        delta = 0\n","        for s in states:\n","            v = value_function[s]\n","            # Bellman Optimality Equation\n","            value_function[s] = max([sum(transition_prob[s, a, s_next] *\n","                                         (rewards[s, a, s_next] + gamma * value_function[s_next])\n","                                         for s_next in states) for a in actions])\n","            delta = max(delta, abs(v - value_function[s]))\n","\n","        if delta < epsilon:\n","            break\n","\n","    # Extract the optimal policy from the optimal value function\n","    optimal_policy = np.argmax([sum(transition_prob[s, a, s_next] *\n","                                    (rewards[s, a, s_next] + gamma * value_function[s_next])\n","                                    for s_next in states) for a in actions], axis=0)\n","\n","    return optimal_policy, value_function\n","\n","# Example usage:\n","states = [0, 1, 2]\n","actions = [0, 1]  # Assuming two possible actions\n","transition_prob = np.random.rand(len(states), len(actions), len(states))\n","transition_prob /= transition_prob.sum(axis=-1, keepdims=True)\n","rewards = np.random.rand(len(states), len(actions), len(states))\n","gamma = 0.9\n","\n","optimal_policy, optimal_value_function = value_iteration(states, actions, transition_prob, rewards, gamma)\n","print(\"Optimal Policy:\", optimal_policy)\n","print(\"Optimal Value Function:\", optimal_value_function)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-OuiwVteYPv","executionInfo":{"status":"ok","timestamp":1709270733826,"user_tz":-330,"elapsed":576,"user":{"displayName":"MUNTAZIR JAHANGIR (RA2111047010138)","userId":"06765373775699398059"}},"outputId":"3d1d48be-4a26-4ed0-d578-ff85ee0e8258"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy: 0\n","Optimal Value Function: [7.03069893 7.00304534 7.15011949]\n"]}]}]}