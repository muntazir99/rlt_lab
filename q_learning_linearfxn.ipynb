{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Kh0Nh-u747gWRBHwXh8vvGDrv7uuFgwK","timestamp":1712911280504}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FRZabxwkn9k","executionInfo":{"status":"ok","timestamp":1712896275062,"user_tz":-330,"elapsed":6019,"user":{"displayName":"RITWATZ RAJ (RA2111047010130)","userId":"07410701510929621527"}},"outputId":"75b806fa-2b3d-462f-a673-ed52be0eaa14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weights after Q-Learning with Linear Function Approximation:\n","[[21.53390031 17.54031839]\n"," [23.68421053 23.68421053]\n"," [26.31578947 19.08984336]]\n"]}],"source":["import numpy as np\n","\n","class Environment:\n","    def __init__(self):\n","        self.num_states = 3\n","        self.num_actions = 2\n","        self.transition_probs = np.array([[[0.5, 0.5, 0.0], [1.0, 0.0, 0.0]],\n","                                          [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]],\n","                                          [[0.0, 1.0, 0.0], [0.5, 0.5, 0.0]]])\n","        self.rewards = np.array([[1.0, 2.0], [0.0, 0.0], [5.0, -1.0]])\n","\n","class QLearningWithLinearFunctionApproximation:\n","    def __init__(self, env, feature_dim, alpha=0.1, epsilon=0.1, discount_factor=0.9):\n","        self.env = env\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.discount_factor = discount_factor\n","        self.weights = np.zeros((feature_dim, env.num_actions))\n","\n","    def featurize_state(self, state):\n","        # For simplicity, we use a simple identity featurization here\n","        return np.eye(self.env.num_states)[state]\n","\n","    def choose_action(self, state):\n","        if np.random.rand() < self.epsilon:\n","            return np.random.choice(self.env.num_actions)\n","        else:\n","            state_features = self.featurize_state(state)\n","            values = np.dot(state_features, self.weights)\n","            return np.argmax(values)\n","\n","    def update(self, state, action, reward, next_state):\n","        state_features = self.featurize_state(state)\n","        next_state_features = self.featurize_state(next_state)\n","        q_values = np.dot(state_features, self.weights)\n","        next_q_values = np.dot(next_state_features, self.weights)\n","        td_target = reward + self.discount_factor * np.max(next_q_values)\n","        td_error = td_target - q_values[action]\n","        self.weights[:, action] += self.alpha * td_error * state_features\n","\n","# Example usage of Q-Learning with Linear Function Approximation\n","env = Environment()\n","feature_dim = env.num_states  # Number of features is equal to the number of states in this example\n","q_learning = QLearningWithLinearFunctionApproximation(env, feature_dim)\n","num_episodes = 1000\n","for _ in range(num_episodes):\n","    state = np.random.randint(env.num_states)\n","    while True:\n","        action = q_learning.choose_action(state)\n","        next_state = np.random.choice(env.num_states, p=env.transition_probs[state, action])\n","        reward = env.rewards[state, action]\n","        q_learning.update(state, action, reward, next_state)\n","        if next_state == 0:\n","            break\n","        state = next_state\n","print(\"Weights after Q-Learning with Linear Function Approximation:\")\n","print(q_learning.weights)\n"]}]}