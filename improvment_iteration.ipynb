{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHXrDw9e9dAJxjfAdAjsh8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqtr_UTvkLMb","executionInfo":{"status":"ok","timestamp":1709876772811,"user_tz":-330,"elapsed":413,"user":{"displayName":"MUNTAZIR JAHANGIR (RA2111047010138)","userId":"06765373775699398059"}},"outputId":"b9e73f01-46de-479e-eeae-d21eea7774f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Value Function:\n","[[-1.0989011 -1.0989011 -1.0989011  9.9010989]\n"," [-1.0989011 -1.0989011 -1.0989011 -1.0989011]\n"," [-1.0989011 -1.0989011 -1.0989011 -1.0989011]\n"," [-1.0989011 -1.0989011 -1.0989011 -1.0989011]]\n","\n","Optimal Policy:\n","[[[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [1. 0. 0. 0.]]]\n"]}],"source":["import numpy as np\n","\n","# Gridworld environment definition\n","GRID_SIZE = 4\n","GAMMA = 0.9  # Discount factor\n","\n","# Rewards\n","REWARD = -1  # Step reward\n","GOAL_REWARD = 10  # Reward for reaching the goal state\n","\n","# Transition probabilities\n","TRANS_PROB = {\n","    0: {(0, 0): 0.9, (0, 1): 0.1, (1, 0): 0.0, (0, 0): 0.0},  # Up\n","    1: {(0, 1): 0.1, (0, 2): 0.8, (0, 1): 0.1, (1, 1): 0.0},  # Right\n","    2: {(1, 0): 0.0, (2, 0): 0.1, (1, 1): 0.9, (1, 0): 0.0},  # Down\n","    3: {(0, 0): 0.0, (0, 0): 0.0, (0, 1): 0.1, (1, 0): 0.9}   # Left\n","}\n","# Actions\n","ACTIONS = [0, 1, 2, 3]  # Up, Right, Down, Left\n","GOAL_STATE = (0, 3)  # Goal state\n","\n","# Helper function to initialize the value function\n","def initialize_value_function():\n","    return np.zeros((GRID_SIZE, GRID_SIZE))\n","\n","# Helper function to calculate the expected future reward\n","def calculate_expected_reward(state, value_function, policy):\n","    row, col = state\n","    expected_reward = 0\n","    for action in ACTIONS:\n","        next_state_probs = TRANS_PROB[action]\n","        reward = REWARD\n","        if (row, col) == GOAL_STATE:\n","            reward = GOAL_REWARD\n","        expected_reward += policy[row, col, action] * (reward + GAMMA * sum([prob * value_function[next_row, next_col]\n","                                                                             for (next_row, next_col), prob in next_state_probs.items()]))\n","    return expected_reward\n","\n","# Policy Evaluation\n","def policy_evaluation(policy, value_function, num_iterations=1000, theta=1e-8):\n","    for i in range(num_iterations):\n","        delta = 0\n","        for row in range(GRID_SIZE):\n","            for col in range(GRID_SIZE):\n","                state = (row, col)\n","                old_value = value_function[row, col]\n","                value_function[row, col] = calculate_expected_reward(state, value_function, policy)\n","                delta = max(delta, abs(old_value - value_function[row, col]))\n","        if delta < theta:\n","            break\n","    return value_function\n","\n","# Policy Improvement\n","def policy_improvement(value_function):\n","    policy = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n","    for row in range(GRID_SIZE):\n","        for col in range(GRID_SIZE):\n","            state = (row, col)\n","            action_values = []\n","            for action in ACTIONS:\n","                next_state_probs = TRANS_PROB[action]\n","                action_value = REWARD\n","                if (row, col) == GOAL_STATE:\n","                    action_value = GOAL_REWARD\n","                action_value += GAMMA * sum(\n","                    [prob * value_function[next_row, next_col] for (next_row, next_col), prob in next_state_probs.items()])\n","                action_values.append(action_value)\n","            best_action = np.argmax(action_values)\n","            policy[row, col, best_action] = 1.0\n","    return policy\n","\n","# Policy Iteration\n","def policy_iteration(num_iterations=1000):\n","    value_function = initialize_value_function()\n","    policy = np.ones((GRID_SIZE, GRID_SIZE, len(ACTIONS))) / len(ACTIONS)  # Initialize with a random policy\n","\n","    for i in range(num_iterations):\n","        value_function = policy_evaluation(policy, value_function)\n","        new_policy = policy_improvement(value_function)\n","\n","        if np.array_equal(policy, new_policy):\n","            break\n","\n","        policy = new_policy\n","\n","    return value_function, policy\n","\n","# Example usage\n","value_function, optimal_policy = policy_iteration()\n","print(\"Value Function:\")\n","print(value_function)\n","print(\"\\nOptimal Policy:\")\n","print(optimal_policy)"]}]}